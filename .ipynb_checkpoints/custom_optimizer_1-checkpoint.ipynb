{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x8zJkCpmj1m"
   },
   "source": [
    "Resources#\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/cc46dc45e1b4b2a9ffab4ad5442f8b864148e45a/torch/optim/_functional.py#L156\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n",
    "https://discuss.pytorch.org/t/custom-loss-functions/29387\n",
    "\n",
    "https://discuss.pytorch.org/t/good-way-to-calculate-element-wise-difference-between-two-models-of-the-same-structure/67592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "iTDCRqOGmorM"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "fxJ9QbZCVq-j"
   },
   "outputs": [],
   "source": [
    "def convert_model_to_param_list(model):\n",
    "    '''\n",
    "    num_of_param=0\n",
    "    for param in model.state_dict().values():\n",
    "        num_of_param += torch.numel(param)\n",
    "    \n",
    "\n",
    "    params=torch.ones([num_of_param])\n",
    "    '''\n",
    "    if torch.typename(model)!='OrderedDict':\n",
    "        model = model.state_dict()\n",
    "\n",
    "    idx=0\n",
    "    params_to_copy_group=[]\n",
    "    for name, param in model.items():\n",
    "        num_params_to_copy = torch.numel(param)\n",
    "        params_to_copy_group.append(param.reshape([num_params_to_copy]).clone().detach())\n",
    "        idx+=num_params_to_copy\n",
    "\n",
    "    params=torch.ones([idx])\n",
    "    idx=0\n",
    "    for param in params_to_copy_group:    \n",
    "        for par in param:\n",
    "            params[idx].copy_(par)\n",
    "            idx += 1\n",
    "\n",
    "    return params\n",
    "\n",
    "def cos_calc_btn_grads(l1, l2):\n",
    "    return torch.dot(l1, l2)/(torch.linalg.norm(l1)+1e-9)/(torch.linalg.norm(l2)+1e-9)\n",
    "\n",
    "\n",
    "def cos_calc(n1, n2):\n",
    "    l1 = convert_model_to_param_list(n1)\n",
    "    l2 = convert_model_to_param_list(n2)\n",
    "    return cos_calc_btn_grads(l1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "n8iAonfWm5ev"
   },
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, name=None, created_time=None, is_malicious=False, net_id=-1):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.created_time = created_time\n",
    "        self.name=name\n",
    "\n",
    "        self.is_malicious=is_malicious\n",
    "        self.net_id=net_id\n",
    "\n",
    "\n",
    "    def save_stats(self, epoch, loss, acc):\n",
    "        self.stats['epoch'].append(epoch)\n",
    "        self.stats['loss'].append(loss)\n",
    "        self.stats['acc'].append(acc)\n",
    "\n",
    "    def copy_params(self, state_dict, coefficient_transfer=100):\n",
    "\n",
    "        own_state = self.state_dict()\n",
    "\n",
    "        for name, param in state_dict.items():\n",
    "            if name in own_state:\n",
    "                shape = param.shape\n",
    "                own_state[name].copy_(param.clone())\n",
    "    \n",
    "    def set_param_to_zero(self):\n",
    "        own_state = self.state_dict()\n",
    "\n",
    "        for name, param in own_state.items():\n",
    "            shape = param.shape\n",
    "            param.mul_(0)       \n",
    "\n",
    "    def aggregate(self, state_dicts, aggr_weights=None):\n",
    "        #self.copy_params(state_dicts[0])\n",
    "        own_state = self.state_dict()\n",
    "        \n",
    "        nw = len(state_dicts)\n",
    "        if aggr_weights is None:\n",
    "            aggr_weights = [1/nw]*nw\n",
    "\n",
    "        for i, state_dict in enumerate(state_dicts):\n",
    "            for name, param in state_dict.items():\n",
    "                if name in own_state:\n",
    "                    shape = param.shape\n",
    "                    own_state[name].add_(param.clone().mul_(aggr_weights[i]))\n",
    "\n",
    "    def calc_grad(self, state_dict, change_self=True):\n",
    "        if change_self:\n",
    "            own_state = self.state_dict()\n",
    "\n",
    "            for name, param in state_dict.items():\n",
    "                if name in own_state:\n",
    "                    shape = param.shape\n",
    "                    own_state[name].sub_(param.clone())\n",
    "        else:\n",
    "            self_params = convert_model_to_param_list(self)\n",
    "            ref_params = convert_model_to_param_list(state_dict)\n",
    "\n",
    "            self_params.sub_(ref_params)\n",
    "            self.grad_params = self_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "kADMGZrmm8PH"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class MnistNet(SimpleNet):#model for mnist\n",
    "    def __init__(self, name=None, created_time=None,num_of_classes = 10):\n",
    "        super(MnistNet, self).__init__(f'{name}_Simple', created_time)\n",
    "     \n",
    "        self.fc_layer = torch.nn.Sequential(#1 * 28 * 28\n",
    "            Flatten(),#784\n",
    "            nn.Linear(784, num_of_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "     \n",
    "        out = self.fc_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "h2FR35-PASFF"
   },
   "outputs": [],
   "source": [
    "class FLNet(SimpleNet):\n",
    "    def __init__(self, name=None, created_time=None,num_of_classes = 10):\n",
    "        super(FLNet, self).__init__(f'{name}_Simple', created_time)\n",
    "\n",
    "        self.mnistnet1 = MnistNet()\n",
    "\n",
    "        self.mnistnet2 = MnistNet()\n",
    "\n",
    "        self.mnistnetavg = MnistNet()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.mnistnet1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "-Bm2SNGGCPOA",
    "outputId": "e912d6ab-0fd6-43c0-b174-fd703750762c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass CNN(SimpleNet):\\n    def __init__(self, name=None, created_time=None,num_of_classes = 10,network_id=-1, net_id=-1, is_malicious=False):\\n        super(CNN, self).__init__(f'{name}_Simple', created_time, is_malicious=is_malicious, net_id=net_id)\\n        self.conv1 = nn.Conv2d(3, 6, 5)\\n        self.pool = nn.MaxPool2d(2, 2)\\n        self.conv2 = nn.Conv2d(6, 16, 5)\\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\\n        self.fc2 = nn.Linear(120, 84)\\n        self.fc3 = nn.Linear(84, 10)\\n\\n    def forward(self, x):\\n        x = self.pool(F.relu(self.conv1(x)))\\n        x = self.pool(F.relu(self.conv2(x)))\\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\\n        x = F.relu(self.fc1(x))\\n        x = F.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\n\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class CNN(SimpleNet):\n",
    "    def __init__(self, name=None, created_time=None,num_of_classes = 10,network_id=-1, net_id=-1, is_malicious=False):\n",
    "        super(CNN, self).__init__(f'{name}_Simple', created_time, is_malicious=is_malicious, net_id=net_id)\n",
    "\n",
    "        self.network_id=network_id\n",
    "\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output    # return x for visualization\n",
    "'''\n",
    "class CNN(SimpleNet):\n",
    "    def __init__(self, name=None, created_time=None,num_of_classes = 10,network_id=-1, net_id=-1, is_malicious=False):\n",
    "        super(CNN, self).__init__(f'{name}_Simple', created_time, is_malicious=is_malicious, net_id=net_id)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "wY6ycM7q9m2A"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def add_pixel_pattern(ori_image):\n",
    "    image = copy.deepcopy(ori_image)\n",
    "    poison_patterns= poison_dict['poison_pattern']\n",
    "    delta =  poison_dict['poison_delta']\n",
    "\n",
    "    for i in range(0, len(poison_patterns)):\n",
    "        pos = poison_patterns[i]\n",
    "        image[0][pos[0]][pos[1]] = min( image[0][pos[0]][pos[1]] + delta/np.sqrt(len(poison_patterns)), 1)\n",
    "\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_poison_batch(bptt,adversarial_index=-1, evaluation=False, attack_type='label_flip'):\n",
    "    if attack_type=='backdoor':\n",
    "        import sys\n",
    "        print(\"still backdooring\")\n",
    "        sys.exit()\n",
    "\n",
    "        images, targets = bptt\n",
    "\n",
    "        poison_count= 0\n",
    "        new_images=images\n",
    "        new_targets=targets\n",
    "\n",
    "        for index in range(0, len(images)):\n",
    "            if evaluation: # poison all data when testing\n",
    "                new_targets[index] = poison_dict['poison_label_swap']\n",
    "                new_images[index] = add_pixel_pattern(images[index])\n",
    "                poison_count+=1\n",
    "\n",
    "            else: # poison part of data when training\n",
    "                if index < poison_dict['poisoning_per_batch']:\n",
    "                    new_targets[index] = poison_dict['poison_label_swap']\n",
    "                    new_images[index] = add_pixel_pattern(images[index])\n",
    "                    poison_count += 1\n",
    "                else:\n",
    "                    new_images[index] = images[index]\n",
    "                    new_targets[index]= targets[index]\n",
    "\n",
    "        new_images = new_images.to(device)\n",
    "        new_targets = new_targets.to(device).long()\n",
    "        if evaluation:\n",
    "            new_images.requires_grad_(False)\n",
    "            new_targets.requires_grad_(False)\n",
    "        return new_images,new_targets,poison_count\n",
    "    \n",
    "    elif attack_type=='degrade' or attack_type=='label_flip':\n",
    "        images, targets = bptt\n",
    "\n",
    "        poison_count= 0\n",
    "        new_images=images\n",
    "        new_targets=targets\n",
    "\n",
    "        num_of_classes = 10\n",
    "\n",
    "        for index in range(0, len(images)):\n",
    "            if evaluation: # poison all data when testing\n",
    "                if attack_type=='degrade':\n",
    "                    new_targets[index] = poison_dict['poison_label_swap']\n",
    "                elif attack_type=='label_flip':\n",
    "                    new_targets[index] = (targets[index]+1)%num_of_classes\n",
    "                new_images[index] = images[index]\n",
    "                poison_count+=1\n",
    "\n",
    "            else: # poison part of data when training\n",
    "                if index < poison_dict['poisoning_per_batch']:\n",
    "                    new_targets[index] = poison_dict['poison_label_swap']\n",
    "                    new_images[index] = add_pixel_pattern(images[index])\n",
    "                    poison_count += 1\n",
    "                else:\n",
    "                    new_images[index] = images[index]\n",
    "                    new_targets[index]= targets[index]\n",
    "\n",
    "        new_images = new_images.to(device)\n",
    "        new_targets = new_targets.to(device).long()\n",
    "        if evaluation:\n",
    "            new_images.requires_grad_(False)\n",
    "            new_targets.requires_grad_(False)\n",
    "        return new_images,new_targets,poison_count\n",
    "        \n",
    "\n",
    "def get_batch(bptt, evaluation=False):\n",
    "    data, target = bptt\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    if evaluation:\n",
    "        data.requires_grad_(False)\n",
    "        target.requires_grad_(False)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "nOhHhkNjE6tb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, Optional\n",
    "\n",
    "from tensorflow import print as prnt\n",
    "\n",
    "def sgd(params: List[Tensor],\n",
    "        d_p_list: List[Tensor],\n",
    "        momentum_buffer_list: List[Optional[Tensor]],\n",
    "        ref_params: List[Tensor],\n",
    "        ref_grad_params: List[Tensor],\n",
    "        *,\n",
    "        weight_decay: float,\n",
    "        momentum: float,\n",
    "        lr: float,\n",
    "        dampening: float,\n",
    "        nesterov: bool,\n",
    "        maximize: bool,\n",
    "        inertia: float,\n",
    "        minimizeDist: bool):\n",
    "    r\"\"\"Functional API that performs SGD algorithm computation.\n",
    "    See :class:`~torch.optim.SGD` for details.\n",
    "    \"\"\"\n",
    "    #print(\"dummy\", len(params), len(ref_params))\n",
    "\n",
    "    if len(ref_params)!=0 and len(params)!=len(ref_params):\n",
    "        #print(params, ref_params)\n",
    "        print('params', params, '\\n')\n",
    "        print('ref_params', ref_params, '\\n')\n",
    "        sys.exit()\n",
    "\n",
    "    if len(ref_grad_params)!=0 and len(params)!=len(ref_grad_params):\n",
    "        print('params', params, '\\n')\n",
    "        print('ref_params', ref_grad_params, '\\n')\n",
    "        sys.exit()\n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        d_p = d_p_list[i]\n",
    "        if weight_decay != 0:\n",
    "            d_p = d_p.add(param, alpha=weight_decay)\n",
    "\n",
    "        if momentum != 0:\n",
    "            buf = momentum_buffer_list[i]\n",
    "\n",
    "            if buf is None:\n",
    "                buf = torch.clone(d_p).detach()\n",
    "                momentum_buffer_list[i] = buf\n",
    "            else:\n",
    "                buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "\n",
    "            if nesterov:\n",
    "                d_p = d_p.add(buf, alpha=momentum)\n",
    "            else:\n",
    "                d_p = buf\n",
    "        \n",
    "        if ref_params is not None and len(ref_params) != 0 and minimizeDist:\n",
    "            diff = 0\n",
    "            ### if minimize euclidean distance\n",
    "            #diff = param - ref_params[i]\n",
    "            ### if minimize cosine similarity, ref_params contains the gradient of the reference network weights\n",
    "            \n",
    "            if ref_grad_params is not None:\n",
    "\n",
    "                dir_sign = torch.sign(ref_params[i]-param)\n",
    "                diff -= ref_params[i]-param\n",
    "                diff -= ref_grad_params[i]\n",
    "                diff = diff.mul_(0.5)\n",
    "            \n",
    "            #d_p = d_p.mul_(0)\n",
    "            d_p = d_p.add(diff, alpha=inertia)\n",
    "        \n",
    "        #if diff_total==0:\n",
    "            #print('diff_total 0')\n",
    "\n",
    "        alpha = lr if maximize else -lr\n",
    "        param.add_(d_p, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "BMvAKmTlFlSV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#from torch.optim import _functional as F\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, ref_param_groups=None, ref_grad_param_groups=None, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False, *, maximize=False, inertia=1.0, minimizeDist=False):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov, maximize=maximize)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        #self.ref_param_groups = ref_param_groups\n",
    "        super(SGD, self).__init__(params, defaults)\n",
    "        #self.ref_param_groups = ref_param_groups\n",
    "        self.inertia=inertia\n",
    "        self.minimizeDist=minimizeDist\n",
    "\n",
    "        if ref_param_groups is not None:\n",
    "            self.__setrefparams__(ref_param_groups)\n",
    "        else:\n",
    "            self.ref_param_groups = ref_param_groups\n",
    "\n",
    "        if ref_grad_param_groups is not None:\n",
    "            self.__setrefgradparams__(ref_grad_param_groups)\n",
    "        else:\n",
    "            self.ref_grad_param_groups = ref_param_groups\n",
    "\n",
    "    def filter_ref_param_group(self, param_group):\n",
    "        r\"\"\"Add a param group to the :class:`Optimizer` s `param_groups`.\n",
    "        This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
    "        trainable and added to the :class:`Optimizer` as training progresses.\n",
    "        Args:\n",
    "            param_group (dict): Specifies what Tensors should be optimized along with group\n",
    "                specific optimization options.\n",
    "        \"\"\"\n",
    "        assert isinstance(param_group, dict), \"param group must be a dict\"\n",
    "\n",
    "        params = param_group['params']\n",
    "        if isinstance(params, torch.Tensor):\n",
    "            param_group['params'] = [params]\n",
    "        elif isinstance(params, set):\n",
    "            raise TypeError('optimizer parameters need to be organized in ordered collections, but '\n",
    "                            'the ordering of tensors in sets will change between runs. Please use a list instead.')\n",
    "        else:\n",
    "            param_group['params'] = list(params)\n",
    "\n",
    "        for param in param_group['params']:\n",
    "            if not isinstance(param, torch.Tensor):\n",
    "                raise TypeError(\"optimizer can only optimize Tensors, \"\n",
    "                                \"but one of the params is \" + torch.typename(param))\n",
    "            if not param.is_leaf:\n",
    "                raise ValueError(\"can't optimize a non-leaf Tensor\")\n",
    "        '''\n",
    "        for name, default in self.defaults.items():\n",
    "            if default is required and name not in param_group:\n",
    "                raise ValueError(\"parameter group didn't specify a value of required optimization parameter \" +\n",
    "                                name)\n",
    "            else:\n",
    "                param_group.setdefault(name, default)\n",
    "        '''\n",
    "\n",
    "        params = param_group['params']\n",
    "        if len(params) != len(set(params)):\n",
    "            warnings.warn(\"optimizer contains a parameter group with duplicate parameters; \"\n",
    "                          \"in future, this will cause an error; \"\n",
    "                          \"see github.com/pytorch/pytorch/issues/40967 for more information\", stacklevel=3)\n",
    "\n",
    "        param_set = set()\n",
    "        for group in self.param_groups:\n",
    "            param_set.update(set(group['params']))\n",
    "\n",
    "        if not param_set.isdisjoint(set(param_group['params'])):\n",
    "            raise ValueError(\"some parameters appear in more than one parameter group\")\n",
    "\n",
    "        #self.ref_param_groups.append(param_group)\n",
    "        return param_group\n",
    "    \n",
    "    def __setrefparams__(self, params):\n",
    "        self.ref_param_groups = []\n",
    "\n",
    "        param_groups = list(params)\n",
    "        '''\n",
    "        if len(param_groups) == 0:\n",
    "            raise ValueError(\"optimizer got an empty parameter list\")\n",
    "        '''\n",
    "        if not isinstance(param_groups[0], dict):\n",
    "            param_groups = [{'params': param_groups}]\n",
    "\n",
    "        for param_group in param_groups:\n",
    "            self.ref_param_groups.append(self.filter_ref_param_group(param_group))\n",
    "\n",
    "    def __setrefgradparams__(self, params):\n",
    "        self.ref_grad_param_groups = []\n",
    "\n",
    "        param_groups = list(params)\n",
    "        '''\n",
    "        if len(param_groups) == 0:\n",
    "            raise ValueError(\"optimizer got an empty parameter list\")\n",
    "        '''\n",
    "        if not isinstance(param_groups[0], dict):\n",
    "            param_groups = [{'params': param_groups}]\n",
    "\n",
    "        for param_group in param_groups:\n",
    "            self.ref_grad_param_groups.append(self.filter_ref_param_group(param_group))\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(SGD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "            group.setdefault('maximize', False)\n",
    "    \n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        \n",
    "        #for group in self.ref_param_groups:\n",
    "            #for p in group['params']:\n",
    "\n",
    "        if self.ref_param_groups is not None and len(self.param_groups)!=len(self.ref_param_groups):\n",
    "            print(len(self.param_groups), len(self.ref_param_groups))\n",
    "            sys.exit()\n",
    "\n",
    "        if self.ref_grad_param_groups is not None and len(self.param_groups)!=len(self.ref_grad_param_groups):\n",
    "            print(len(self.param_groups), len(self.ref_grad_param_groups))\n",
    "            sys.exit()\n",
    "        for i in range(len(self.param_groups)):\n",
    "            group = self.param_groups[i]\n",
    "            if self.ref_param_groups is not None:\n",
    "                ref_group = self.ref_param_groups[i]\n",
    "            if self.ref_grad_param_groups is not None:\n",
    "                ref_grad_group = self.ref_grad_param_groups[i]\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "            momentum_buffer_list = []\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            maximize = group['maximize']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        momentum_buffer_list.append(None)\n",
    "                    else:\n",
    "                        momentum_buffer_list.append(state['momentum_buffer'])\n",
    "            ref_params=[]\n",
    "            ref_grad_params=[]\n",
    "            if self.ref_param_groups is not None:\n",
    "                for p in ref_group['params']:\n",
    "                    ref_params.append(p)\n",
    "            if self.ref_grad_param_groups is not None:\n",
    "                for p in ref_grad_group['params']:\n",
    "                    ref_grad_params.append(p)\n",
    "            \n",
    "            sgd(params_with_grad,\n",
    "                  d_p_list,\n",
    "                  momentum_buffer_list,\n",
    "                  ref_params=ref_params,\n",
    "                  ref_grad_params=ref_grad_params,\n",
    "                  weight_decay=weight_decay,\n",
    "                  momentum=momentum,\n",
    "                  lr=lr,\n",
    "                  dampening=dampening,\n",
    "                  nesterov=nesterov,\n",
    "                  maximize=maximize,\n",
    "                  inertia=self.inertia,\n",
    "                  minimizeDist=self.minimizeDist)\n",
    "            \n",
    "\n",
    "            # update momentum_buffers in state\n",
    "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "                state = self.state[p]\n",
    "                state['momentum_buffer'] = momentum_buffer\n",
    "\n",
    "        return loss\n",
    "\n",
    "    ##  add add_param method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "NDJtZrEdRwht"
   },
   "outputs": [],
   "source": [
    "def new_loss(output, target):\n",
    "    loss = F.nll_loss(output, target)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "yWy7CtC7GpPG"
   },
   "outputs": [],
   "source": [
    "def inv_grad_test(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, torch.isfinite(param.grad).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "I3r_mO-LDaJ6"
   },
   "outputs": [],
   "source": [
    "loss_func=nn.CrossEntropyLoss()\n",
    "\n",
    "def train(network, optimizer, epoch):\n",
    "  network.train()\n",
    "\n",
    "  if network.network_id!=-1:\n",
    "      (_, temp_train_loader)=train_loaders[network.network_id]\n",
    "  else:\n",
    "      temp_train_loader=train_loader\n",
    "\n",
    "  for batch_idx, (data, target) in enumerate(temp_train_loader):\n",
    "    if network.network_id>=1:\n",
    "        data, target, poison_num = get_poison_batch((data, target))\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = loss_func(output, target)\n",
    "    loss.backward()\n",
    "    #inv_grad_test(network)\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "      epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "      100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(network.state_dict(), 'model.pth')\n",
    "      torch.save(optimizer.state_dict(), 'optimizer.pth')\n",
    "  if network.network_id>=1:\n",
    "      sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "rePZEDgTs5Zg"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_net(network, optimizer, trainloader, epoch, poisonNow=False, print_flag=False, attack_type='backdoor'):\n",
    "  for batch_idx, (data, target) in enumerate(tqdm(trainloader)):\n",
    "    if poisonNow:\n",
    "        data, target, poison_num = get_poison_batch((data, target), attack_type=attack_type)\n",
    "    else:\n",
    "        data, target = get_batch((data, target))\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = loss_func(output, target)\n",
    "    loss.backward()\n",
    "    #inv_grad_test(network)\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "        if print_flag:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "            100. * batch_idx / len(trainloader), loss.item()))\n",
    "        train_losses.append(loss.item())\n",
    "        train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "DNbRH73f9IFr"
   },
   "outputs": [],
   "source": [
    "def test(network):\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader):\n",
    "        data, target = get_batch((data, target))\n",
    "        output = network(data)\n",
    "        test_loss += loss_func(output, target).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "  return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "O9mnbnO-BSvs"
   },
   "outputs": [],
   "source": [
    "def backdoor_test(network):\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader):\n",
    "      data, target, poison_num = get_poison_batch((data, target), evaluation=True, attack_type='backdoor')\n",
    "      output = network(data)\n",
    "      test_loss += loss_func(output, target).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nBackdoor Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "_BXauMW5n3r3"
   },
   "outputs": [],
   "source": [
    "def calcDiff(network, network2):\n",
    "    return sum((x - y).abs().sum() for x, y in zip(network.state_dict().values(), network2.state_dict().values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "IB55dWe-UoTd"
   },
   "outputs": [],
   "source": [
    "def get_scaled_up_grads(glob_net, networks, self=None, iter=-1):\n",
    "    nets_grads=[]\n",
    "    clean_server_grad=None\n",
    "    grads=[]\n",
    "\n",
    "    for i in range(len(networks)):\n",
    "        grad_net=CNN().to(device)\n",
    "        grad_net.to(device)\n",
    "        grad_net.copy_params(networks[i].state_dict())\n",
    "        nets_grads.append(grad_net)\n",
    "        \n",
    "        grad_net.calc_grad(glob_net.state_dict())\n",
    "        grads.append(convert_model_to_param_list(grad_net))\n",
    "\n",
    "    scaled_grad=CNN().to(device)\n",
    "    scaled_grad.set_param_to_zero()\n",
    "    scaled_grad.aggregate([n.state_dict() for n in nets_grads], aggr_weights=[-1]*(len(networks)-1)+[len(networks)])\n",
    "\n",
    "    self.log.append((iter, 'Cos_sim btn scaled grad and clean server grad', 'get_scaled_up_grads', cos_calc(scaled_grad, nets_grads[-2])))\n",
    "    print(self.log[-1])\n",
    "    self.log.append((iter, 'Cos_sim btn mal grad and clean server grad', 'get_scaled_up_grads', cos_calc(nets_grads[-1], nets_grads[-2])))\n",
    "    print(self.log[-1])\n",
    "\n",
    "    scaled_grad.aggregate([glob_net.state_dict()], aggr_weights=[1])\n",
    "    return scaled_grad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "py2OvW8FfoTQ"
   },
   "outputs": [],
   "source": [
    "class CustomFL:\n",
    "    def __init__(self, num_of_benign_nets=1, num_of_mal_nets=1, inertia=0.1, n_iter=10,\n",
    "                 n_epochs=3, poison_starts_at_iter=3, learning_rate=0.1, momentum=0, weight_decay=0.1,\n",
    "                 attack_type='label_flip', scale_up=False, minimizeDist=True):\n",
    "        self.global_net = CNN().to(device)\n",
    "        self.global_net_optim = SGD(self.global_net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        self.global_net.to(device)\n",
    "        self.benign_nets = []\n",
    "        self.benign_net_optims = []\n",
    "        for i in range(num_of_benign_nets):\n",
    "            network = CNN(net_id=i)\n",
    "            network.copy_params(self.global_net.state_dict())\n",
    "            optim = SGD(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay, inertia=inertia)\n",
    "            network.to(device)\n",
    "            self.benign_nets.append(network)\n",
    "            self.benign_net_optims.append(optim)\n",
    "        self.mal_nets = []\n",
    "        self.mal_net_optims = []\n",
    "        for i in range(num_of_mal_nets):\n",
    "            network = CNN(is_malicious=True, net_id=i)\n",
    "            network.copy_params(self.global_net.state_dict())\n",
    "            optim = SGD(network.parameters(), lr=learning_rate,\n",
    "                        momentum=momentum, weight_decay=weight_decay, inertia=inertia, minimizeDist=minimizeDist)\n",
    "            network.to(device)\n",
    "            self.mal_nets.append(network)\n",
    "            self.mal_net_optims.append(optim)\n",
    "\n",
    "        self.current_iter=0\n",
    "        self.num_of_benign_nets=num_of_benign_nets\n",
    "        self.num_of_mal_nets=num_of_mal_nets\n",
    "        self.inertia_rate=inertia\n",
    "        self.n_iter=n_iter\n",
    "        self.n_epochs=n_epochs\n",
    "        self.learning_rate=learning_rate\n",
    "        self.momentum=momentum\n",
    "        self.poison_starts_at_iter=poison_starts_at_iter\n",
    "        self.weight_decay=weight_decay\n",
    "        self.attack_type=attack_type\n",
    "        self.scale_up=scale_up\n",
    "\n",
    "        self.log=[]\n",
    "        self.debug_log={}\n",
    "        self.debug_log['cluster']=[]\n",
    "        self.debug_log['coses']=[]\n",
    "        n_nets=num_of_benign_nets+num_of_mal_nets\n",
    "        self.cos_matrix=np.zeros((n_nets, n_nets))\n",
    "\n",
    "    def cluster_grads(self, iter=-1):\n",
    "        nets = self.benign_nets + self.mal_nets\n",
    "        for net in nets:\n",
    "            net.calc_grad(self.global_net.state_dict(), change_self=False)\n",
    "\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        X = [np.array(net.grad_params) for net in nets]\n",
    "        X= np.array(X)\n",
    "        clustering = AgglomerativeClustering(n_clusters=num_of_distributions, affinity='cosine', linkage='complete').fit(X)\n",
    "        from sklearn.metrics.cluster import adjusted_rand_score\n",
    "        print('Original Copylist', copylist)\n",
    "        print('Found clusters', clustering.labels_)\n",
    "        print('Original groups', [np.argwhere(np.array(copylist)==i).flatten() for i in range(num_of_distributions)])\n",
    "        print('Clustered groups', [np.argwhere(clustering.labels_==i).flatten() for i in range(num_of_distributions)])\n",
    "        print('Clustering score', adjusted_rand_score(clustering.labels_.tolist(), copylist))\n",
    "        self.log.append((iter, 'Original copylist', 'cluster_grads', copylist))\n",
    "        self.log.append((iter, 'Clusters', 'cluster_grads', clustering.labels_))\n",
    "        #self.debug_log['cluster'].append((iter, 'Cluster Score', 'cluster_grads', adjusted_rand_score(clustering.labels_.tolist(), copylist)))\n",
    "        \n",
    "        coses=[]\n",
    "        \n",
    "        for i1, net1 in enumerate(nets):\n",
    "            coses_l=[]\n",
    "            for i2, net2 in enumerate(nets):\n",
    "                coses_l.append(cos_calc_btn_grads(net1.grad_params, net2.grad_params))\n",
    "            coses.append(coses_l)\n",
    "            \n",
    "        coses = np.array(coses)\n",
    "        \n",
    "        self.cos_matrix = self.cos_matrix + coses\n",
    "        self.cos_matrix = self.cos_matrix/np.linalg.norm(self.cos_matrix)\n",
    "                    \n",
    "        print(self.cos_matrix)\n",
    "        \n",
    "        clustering = AgglomerativeClustering(n_clusters=num_of_distributions, affinity='precomputed', linkage='complete').fit(1-self.cos_matrix)\n",
    "        print('Original Copylist', copylist)\n",
    "        print('Found clusters', clustering.labels_)\n",
    "        print('Original groups', [np.argwhere(np.array(copylist)==i).flatten() for i in range(num_of_distributions)])\n",
    "        print('Clustered groups', [np.argwhere(clustering.labels_==i).flatten() for i in range(num_of_distributions)])\n",
    "        print('Clustering score', adjusted_rand_score(clustering.labels_.tolist(), copylist))\n",
    "        self.debug_log['cluster'].append((iter, 'Cluster Score', 'cluster_grads', adjusted_rand_score(clustering.labels_.tolist(), copylist)))\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        X = [np.array(net.grad_params) for net in self.benign_nets]\n",
    "        X= np.array(X)\n",
    "        copylist2=copylist[:self.num_of_benign_nets]\n",
    "        clustering = AgglomerativeClustering(n_clusters=len(set(copylist2)), affinity='cosine', linkage='complete').fit(X)\n",
    "        print('Original Copylist', copylist2)\n",
    "        print('Found clusters', clustering.labels_)\n",
    "        print('Original groups', [np.argwhere(np.array(copylist2)==i).flatten() for i in range(num_of_distributions)])\n",
    "        print('Clustered groups', [np.argwhere(clustering.labels_==i).flatten() for i in range(num_of_distributions)])\n",
    "        print('Clustering score', adjusted_rand_score(clustering.labels_.tolist(), copylist2))\n",
    "        '''\n",
    "        \n",
    "        return coses\n",
    "\n",
    "    def FLtrust(self, iter=-1):\n",
    "        clean_server_grad=None\n",
    "        grads=[]\n",
    "        nets_grads=[]\n",
    "        \n",
    "        nets = self.benign_nets + self.mal_nets\n",
    "        for net in nets:\n",
    "            net.calc_grad(self.global_net.state_dict(), change_self=False)\n",
    "            grad_net = CNN().to(device)\n",
    "            grad_net.to(device)\n",
    "            grad_net.copy_params(net.state_dict())\n",
    "            grad_net.aggregate([self.global_net.state_dict()], aggr_weights=[-1])\n",
    "            nets_grads.append(grad_net)\n",
    "\n",
    "        for i in range(self.num_of_benign_nets):\n",
    "            grads.append(self.benign_nets[i].grad_params)\n",
    "            if i==self.num_of_benign_nets-1:\n",
    "                clean_server_grad=grads[i]\n",
    "\n",
    "        for i in range(self.num_of_mal_nets):\n",
    "            grads.append(self.mal_nets[i].grad_params)\n",
    "        \n",
    "        norms = [torch.linalg.norm(grad) for grad in grads]\n",
    "        print('Norms of local gradients ', norms)\n",
    "        self.log.append((iter, 'Norms of local gradients ', 'FLTrust', norms))\n",
    "\n",
    "        \n",
    "        cos_sims=[cos_calc_btn_grads(grad, clean_server_grad) for grad in grads]\n",
    "\n",
    "        '''\n",
    "        for grad in grads:\n",
    "            cos_sims.append(torch.dot(grad, clean_server_grad)/ (torch.linalg.norm(grad)+ 1e-9) / (torch.linalg.norm(clean_server_grad)+ 1e-9))\n",
    "        '''\n",
    "        print('\\n Aggregating models')\n",
    "\n",
    "        #print([cos_calc() ])\n",
    "\n",
    "        print('Cosine Similarities: ', cos_sims)\n",
    "        self.log.append((iter, 'Cosine Similarities', 'FLtrust', cos_sims))\n",
    "        cos_sims = np.maximum(np.array(cos_sims), 0)\n",
    "        norm_weights = cos_sims/(np.sum(cos_sims)+1e-9)\n",
    "        for i in range(len(norm_weights)):\n",
    "            norm_weights[i] = norm_weights[i] * torch.linalg.norm(clean_server_grad) / (torch.linalg.norm(grads[i]))\n",
    "        \n",
    "        print('Aggregation Weights: ', norm_weights)\n",
    "        self.log.append((iter, 'Aggregation Weights', 'FLtrust', norm_weights))\n",
    "\n",
    "        self.global_net.aggregate([grad.state_dict() for grad in nets_grads], aggr_weights=norm_weights)\n",
    "            \n",
    "            \n",
    "\n",
    "    def train_local_net(self, is_malicious, net_id, iter, ref_net_for_minimizing_dist=None):\n",
    "        if is_malicious:\n",
    "            network=self.mal_nets[net_id]\n",
    "            optim=self.mal_net_optims[net_id]\n",
    "            # will change later to aggregate of benign_nets\n",
    "            if ref_net_for_minimizing_dist is None:\n",
    "                ref_net_for_minimizing_dist = self.benign_nets[0].parameters()\n",
    "            ref_grad, ref_net = ref_net_for_minimizing_dist\n",
    "            if ref_grad is None:\n",
    "                import sys\n",
    "                sys.exit()\n",
    "            if ref_grad is not None:\n",
    "                optim.__setrefgradparams__(ref_grad.parameters())\n",
    "            optim.__setrefparams__(ref_net.parameters())\n",
    "        else:\n",
    "            network=self.benign_nets[net_id]\n",
    "            optim=self.benign_net_optims[net_id]\n",
    "\n",
    "        (_, _, trainloader) = train_loaders[iter][net_id + is_malicious*self.num_of_benign_nets]\n",
    "\n",
    "        poisonNow = True if is_malicious and iter>=self.poison_starts_at_iter else False\n",
    "        for epoch in range(self.n_epochs if not poisonNow else (self.n_epochs*1)):\n",
    "            clientType = 'Malicious' if is_malicious else 'Benign'\n",
    "            print(f'Iter {iter} - Epoch {epoch} - Client Type: {clientType} - Client Number {net_id} - Poison Training {poisonNow}')\n",
    "            train_net(network, optim, trainloader, epoch, poisonNow=poisonNow, attack_type=self.attack_type)\n",
    "        network.calc_grad(self.global_net.state_dict(), change_self=False)\n",
    "        if poisonNow:\n",
    "            acc=test(network)\n",
    "            self.log.append((iter, 'Local net test accuracy: mal', 'train_local_net', acc))\n",
    "            if self.attack_type=='backdoorq':\n",
    "                acc = backdoor_test(network)\n",
    "                self.log.append((iter, 'Local net backdoor test accuracy: mal', 'train_local_net', acc))\n",
    "\n",
    "    def train(self):\n",
    "        n_epochs=self.n_epochs\n",
    "        self.n_epochs=1\n",
    "        for iter in range(self.n_iter):\n",
    "            distanceList=[]\n",
    "            cosList=[]\n",
    "            networks=[]\n",
    "            networks+=self.benign_nets\n",
    "            networks+=self.mal_nets\n",
    "            \n",
    "            for epoch in range(n_epochs):\n",
    "                \n",
    "                print(f'\\n\\n Entering epoch {epoch}')\n",
    "\n",
    "                for i in range(self.num_of_benign_nets):\n",
    "                    self.train_local_net(False, i, iter)\n",
    "                \n",
    "                coses = self.cluster_grads(iter)\n",
    "                \n",
    "                self.debug_log['coses'].append((iter, epoch, coses))\n",
    "\n",
    "            benign_aggr_net=CNN().to(device)\n",
    "            benign_aggr_net.set_param_to_zero()\n",
    "\n",
    "            ### if adversary knows benign_net_aggregates\n",
    "            benign_aggr_net.aggregate([net.state_dict() for net in self.benign_nets])\n",
    "            ### if adversary knows clean server\n",
    "            #benign_aggr_net.copy_params(self.benign_nets[-1].state_dict())\n",
    "\n",
    "            benign_aggr_net_grad=CNN().to(device)\n",
    "            benign_aggr_net_grad.copy_params(benign_aggr_net.state_dict())\n",
    "            benign_aggr_net_grad.aggregate([self.global_net.state_dict()], aggr_weights=[-1])\n",
    "\n",
    "            \n",
    "            for i in range(self.num_of_mal_nets):\n",
    "                self.train_local_net(True, i, iter, ref_net_for_minimizing_dist=(benign_aggr_net_grad, benign_aggr_net))\n",
    "                \n",
    "                if self.scale_up:\n",
    "                    scaled_up_grad = get_scaled_up_grads(self.global_net, networks, self, iter)\n",
    "                    self.mal_nets[i].copy_params(scaled_up_grad.state_dict())\n",
    "                    #self.mal_nets[i].aggregate([benign_aggr_net.state_dict()])\n",
    "                \n",
    "\n",
    "            cosList=[cos_calc_btn_grads(net.grad_params, self.benign_nets[-1].grad_params) for net in networks]\n",
    "            distanceList=[calcDiff(net, self.benign_nets[-1]) for net in networks]\n",
    "\n",
    "            #self.cluster_grads()\n",
    "\n",
    "            self.log.append((iter, 'Benign net distance', 'train', distanceList[:self.num_of_benign_nets]))\n",
    "            print('Benign net distance', distanceList[:self.num_of_benign_nets])\n",
    "            self.log.append((iter, 'Malicious net distance', 'train', distanceList[self.num_of_benign_nets:]))\n",
    "            print('Malicious net distance', distanceList[self.num_of_benign_nets:])\n",
    "            self.log.append((iter, 'Cos sim list', 'train', cosList))\n",
    "            print('cos_sim list ', cosList)\n",
    "\n",
    "            # aggregate nets\n",
    "            #self.global_net.set_param_to_zero()\n",
    "            #self.global_net.aggregate([network.state_dict() for network in networks])\n",
    "            self.FLtrust(iter=iter)\n",
    "            print('\\n\\n\\nAggregate test at iter ', iter)\n",
    "            acc=test(self.global_net)\n",
    "            self.log.append((iter, 'Test accuracy: agg net', 'train', acc))\n",
    "            #backdoor_test(self.global_net)\n",
    "            self.log.append((iter, 'Backdoor test accuracy: agg net', 'train', acc))\n",
    "            self.log.append((iter, 'Distance between aggregate global and clean server', 'train', calcDiff(self.global_net, self.benign_nets[-1])))\n",
    "\n",
    "            # set all local nets equal to global net at the end of the iteration\n",
    "            \n",
    "            for network in networks:\n",
    "                network.copy_params(self.global_net.state_dict())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "IPqR6UKz9G7U"
   },
   "outputs": [],
   "source": [
    "poison_dict = dict()\n",
    "poison_dict['poison_delta'] = 0.1\n",
    "poison_dict['poison_pattern'] = [[23,25], [24,24],[25,23],[25,25]]\n",
    "poison_dict['poisoning_per_batch'] = 80\n",
    "poison_dict['poison_label_swap'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szKjrPoXvw_x",
    "outputId": "f9875642-8768-4770-8b3a-57106ae85412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 1] 6\n",
      "[1, 0, 0, 3, 2, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c2913e2250>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "import torchvision\n",
    "import random\n",
    "\n",
    "batch_size_train = 100\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "\n",
    "\n",
    "### important hyperparameters\n",
    "num_of_workers=6\n",
    "num_of_mal_workers=0\n",
    "n_iter=50\n",
    "n_epochs=5\n",
    "poison_starts_at_iter=n_iter\n",
    "inertia=0.1\n",
    "momentum=0.1\n",
    "attack_type='label_flip'\n",
    "scale_up=False\n",
    "minimizeDist=False\n",
    "\n",
    "iid = False\n",
    "num_of_distributions = int(num_of_workers/2)\n",
    "num_of_workers_in_distribs = num_of_workers * np.random.dirichlet(np.array(num_of_distributions * [3.0]))\n",
    "num_of_workers_in_distribs = [int(val) for val in num_of_workers_in_distribs]\n",
    "while 0 in num_of_workers_in_distribs:\n",
    "    num_of_workers_in_distribs.remove(0)\n",
    "num_of_workers_in_distribs.append(num_of_workers-sum(num_of_workers_in_distribs))\n",
    "print(num_of_workers_in_distribs, sum(num_of_workers_in_distribs))\n",
    "num_of_distributions = len(num_of_workers_in_distribs)\n",
    "copylist = []\n",
    "for i in range(len(num_of_workers_in_distribs)):\n",
    "    copylist += num_of_workers_in_distribs[i]*[i]\n",
    "random.shuffle(copylist)\n",
    "print(copylist)\n",
    "\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "UoYIstgt9-TH"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "#test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "5WkA_f7UuiOP"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "dataPath = ''\n",
    "\n",
    "import random\n",
    "\n",
    "train_loaders=[]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "    ### if dataset is mnist\n",
    "    transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    ### if dataset is cifar\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST('', train=True, download=True,\n",
    "                               transform=transform)\n",
    "test_dataset = datasets.FashionMNIST('', train=False, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "all_range = list(range(len(train_dataset)))\n",
    "random.shuffle(all_range)\n",
    "\n",
    "def get_train_iid(all_range, model_no, iter_no):\n",
    "    \"\"\"\n",
    "    This method equally splits the dataset.\n",
    "    :param params:\n",
    "    :param all_range:\n",
    "    :param model_no:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    data_len_for_iter = int(len(train_dataset) / n_iter)\n",
    "    data_len = int(data_len_for_iter/num_of_workers)\n",
    "    sub_indices_for_iter = all_range[iter_no * data_len_for_iter: (iter_no + 1) * data_len_for_iter]\n",
    "    sub_indices = sub_indices_for_iter[model_no * data_len: (model_no + 1) * data_len ]\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=batch_size_train,\n",
    "                                        sampler=torch.utils.data.sampler.SubsetRandomSampler(sub_indices)\n",
    "                                        )\n",
    "    return train_loader\n",
    "\n",
    "def get_train_noniid(indices):\n",
    "    \"\"\"\n",
    "    This method is used along with Dirichlet distribution\n",
    "    :param params:\n",
    "    :param indices:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=int(len(train_dataset)/num_of_workers),\n",
    "                                        sampler=torch.utils.data.sampler.SubsetRandomSampler(\n",
    "                                            indices))\n",
    "    return train_loader\n",
    "\n",
    "def poison_test_dataset(test_dataset, batch_size):\n",
    "    logger.info('get poison test loader')\n",
    "    # delete the test data with target label\n",
    "    test_classes = {}\n",
    "    for ind, x in enumerate(test_dataset):\n",
    "        _, label = x\n",
    "        if label in test_classes:\n",
    "            test_classes[label].append(ind)\n",
    "        else:\n",
    "            test_classes[label] = [ind]\n",
    "\n",
    "    range_no_id = list(range(0, len(test_dataset)))\n",
    "    for image_ind in test_classes[poison_dict['poison_label_swap']]:\n",
    "        if image_ind in range_no_id:\n",
    "            range_no_id.remove(image_ind)\n",
    "    poison_label_inds = test_classes[poison_dict['poison_label_swap']]\n",
    "\n",
    "    return torch.utils.data.DataLoader(test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        sampler=torch.utils.data.sampler.SubsetRandomSampler(\n",
    "                            range_no_id)), \\\n",
    "            torch.utils.data.DataLoader(test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        sampler=torch.utils.data.sampler.SubsetRandomSampler(\n",
    "                                            poison_label_inds))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "nprLWVYSlzMe"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sample_dirichlet_train_data(no_participants=num_of_workers, dataset=train_dataset, alpha=0.9, copylist=np.arange(num_of_workers)):\n",
    "    \"\"\"\n",
    "        Input: Number of participants and alpha (param for distribution)\n",
    "        Output: A list of indices denoting data in CIFAR training set.\n",
    "        Requires: dataset_classes, a preprocessed class-indice dictionary.\n",
    "        Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "        dirichlet distribution to sample number of images in each class.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_classes = {}\n",
    "    for ind, x in enumerate(dataset):\n",
    "        _, label = x\n",
    "        #if ind in self.params['poison_images'] or ind in self.params['poison_images_test']:\n",
    "        #    continue\n",
    "        if label in dataset_classes:\n",
    "            dataset_classes[label].append(ind)\n",
    "        else:\n",
    "            dataset_classes[label] = [ind]\n",
    "    class_size = len(dataset_classes[0])\n",
    "    per_participant_list = defaultdict(list)\n",
    "    no_classes = len(dataset_classes.keys())\n",
    "\n",
    "    for n in range(no_classes):\n",
    "        random.shuffle(dataset_classes[n])\n",
    "        num_of_non_iid_participants = len(np.unique(copylist))\n",
    "        sampled_probabilities = np.random.dirichlet(\n",
    "            np.array(num_of_non_iid_participants * [alpha]))\n",
    "        new_list = []\n",
    "        for ip in copylist:\n",
    "            #new_list.append(np.random.normal(loc=sampled_probabilities[ip], scale=0.005))\n",
    "            new_list.append(sampled_probabilities[ip])\n",
    "        sampled_probabilities = class_size * np.array(new_list)/np.sum(np.array(new_list))\n",
    "        sigmas = 0.0 * sampled_probabilities\n",
    "        sampled_probabilities = np.random.normal(sampled_probabilities, scale=sigmas)\n",
    "        print(sampled_probabilities)\n",
    "        for user in range(no_participants):\n",
    "            no_imgs = int(round(sampled_probabilities[user]))\n",
    "            sampled_list = dataset_classes[n][:min(len(dataset_classes[n]), no_imgs)]\n",
    "            per_participant_list[user].extend(sampled_list)\n",
    "            dataset_classes[n] = dataset_classes[n][min(len(dataset_classes[n]), no_imgs):]\n",
    "\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eoJhvrcTwY6O",
    "outputId": "68bbf02a-521b-49df-d5f8-bf85c527128b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1929.71738811 1357.05190663 1357.05190663   16.03945587  670.06967137\n",
      "  670.06967137]\n",
      "[1091.10817647  254.02831758  254.02831758 3325.78350886  537.52583976\n",
      "  537.52583976]\n",
      "[1180.06223292 2008.10534894 2008.10534894   85.21988887  359.25359016\n",
      "  359.25359016]\n",
      "[1433.70829631 1416.64398632 1416.64398632  516.48775363  608.25798871\n",
      "  608.25798871]\n",
      "[ 962.32912534   46.96201111   46.96201111 3556.75182474  693.49751385\n",
      "  693.49751385]\n",
      "[ 879.13742098  187.47881909  187.47881909 2146.85329332 1299.52582376\n",
      " 1299.52582376]\n",
      "[ 882.14061801  741.83528133  741.83528133   23.8655614  1805.16162897\n",
      " 1805.16162897]\n",
      "[3723.28238304  407.41350059  407.41350059  390.29475042  535.79793269\n",
      "  535.79793269]\n",
      "[ 958.73475294 1930.63754467 1930.63754467  515.17696084  332.40659844\n",
      "  332.40659844]\n",
      "[ 275.26503188  493.30404343  493.30404343  699.70315706 2019.2118621\n",
      " 2019.2118621 ]\n",
      "[[(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)], [(-1, 0, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3148>), (-1, 1, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3D48>), (-1, 2, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3888>), (-1, 3, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B36C8>), (-1, 4, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3E08>), (-1, 5, <torch.utils.data.dataloader.DataLoader object at 0x000001C2E11B3C48>)]]\n"
     ]
    }
   ],
   "source": [
    "if iid:\n",
    "    train_loaders=[]\n",
    "    for i in range(n_iter):\n",
    "        train_loaders.append([(i, pos, get_train_iid(all_range, pos, i))\n",
    "                                for pos in range(num_of_workers)])\n",
    "else:\n",
    "    indices_per_participant = sample_dirichlet_train_data(\n",
    "        num_of_workers,\n",
    "        #dataset= torch.utils.data.Subset(train_dataset, list(range(240))),\n",
    "        alpha=0.95,\n",
    "        copylist=copylist)\n",
    "    train_loaders = [(-1, pos, get_train_noniid(indices)) for pos, indices in\n",
    "                    indices_per_participant.items()]\n",
    "    train_loaders = n_iter * [train_loaders]\n",
    "\n",
    "print(train_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFU41nvozNJI",
    "outputId": "12464ed8-464b-4431-b96f-a69a9d84bf69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Entering epoch 0\n",
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 1 1 2 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([3], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[ 0.2985419   0.12356381  0.12335172  0.11622188  0.05132831  0.04969571]\n",
      " [ 0.12356381  0.2985429   0.29850082 -0.01527772 -0.01617978 -0.01579346]\n",
      " [ 0.12335172  0.29850082  0.29854265 -0.01589095 -0.01657911 -0.01618345]\n",
      " [ 0.11622188 -0.01527772 -0.01589095  0.29854415  0.04418788  0.04324435]\n",
      " [ 0.05132831 -0.01617978 -0.01657911  0.04418788  0.29854126  0.29843361]\n",
      " [ 0.04969571 -0.01579346 -0.01618345  0.04324435  0.29843361  0.29854133]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 1 1 2 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([3], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "\n",
      "\n",
      " Entering epoch 1\n",
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 1 1 3 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[0.28009769 0.09491709 0.09410169 0.0729348  0.07786535 0.07622187]\n",
      " [0.09491709 0.28009824 0.28002951 0.05500122 0.10140751 0.10215153]\n",
      " [0.09410169 0.28002951 0.28009816 0.05521945 0.10159155 0.10234997]\n",
      " [0.0729348  0.05500122 0.05521945 0.28009967 0.11208226 0.1123166 ]\n",
      " [0.07786536 0.10140751 0.10159155 0.11208226 0.28009856 0.27999047]\n",
      " [0.07622187 0.10215153 0.10234997 0.1123166  0.27999047 0.28009904]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 1 1 3 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "\n",
      "\n",
      " Entering epoch 2\n",
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 0 0 3 1 1]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[0.26468288 0.11113544 0.11011781 0.0770825  0.10056602 0.09902834]\n",
      " [0.11113544 0.26468366 0.26457482 0.06874868 0.12095412 0.12169096]\n",
      " [0.11011782 0.26457482 0.26468391 0.0694072  0.12152803 0.12228678]\n",
      " [0.0770825  0.06874869 0.0694072  0.2646845  0.12393785 0.12418976]\n",
      " [0.10056603 0.12095412 0.12152803 0.12393785 0.26468385 0.26457465]\n",
      " [0.09902835 0.12169096 0.12228678 0.12418976 0.26457465 0.2646835 ]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 1 1 3 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "\n",
      "\n",
      " Entering epoch 3\n",
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 0 0 3 1 1]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[0.26398641 0.11615542 0.11447906 0.08839296 0.11229516 0.11057031]\n",
      " [0.11615542 0.26398772 0.26378211 0.06589118 0.11326613 0.11390759]\n",
      " [0.11447906 0.26378211 0.26398775 0.06697951 0.11431957 0.11499925]\n",
      " [0.08839296 0.06589119 0.06697951 0.26398879 0.12558352 0.12564136]\n",
      " [0.11229516 0.11326613 0.11431957 0.12558352 0.26398885 0.26385708]\n",
      " [0.11057031 0.11390759 0.11499925 0.12564136 0.26385708 0.26398764]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 0 0 3 1 1]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "\n",
      "\n",
      " Entering epoch 4\n",
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 0 0 3 1 1]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[0.26440775 0.12145868 0.1190944  0.09665222 0.12070963 0.11888527]\n",
      " [0.12145868 0.26440862 0.26408148 0.06634631 0.10381049 0.10405549]\n",
      " [0.1190944  0.26408148 0.26440817 0.06690131 0.10432235 0.10461317]\n",
      " [0.09665222 0.06634631 0.06690131 0.26440847 0.1254632  0.12517146]\n",
      " [0.12070964 0.10381049 0.10432234 0.1254632  0.2644087  0.26424649]\n",
      " [0.11888528 0.10405549 0.10461318 0.12517146 0.26424649 0.2644077 ]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 0 0 3 1 1]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "Benign net distance [tensor(111.7629), tensor(95.7573), tensor(96.7238), tensor(137.5280), tensor(3.1627), tensor(0.)]\n",
      "Malicious net distance []\n",
      "cos_sim list  [tensor(0.4578), tensor(0.3835), tensor(0.3851), tensor(0.4727), tensor(0.9994), tensor(1.0000)]\n",
      "Norms of local gradients  [tensor(1.1478), tensor(0.8518), tensor(0.8643), tensor(1.5227), tensor(0.7908), tensor(0.7938)]\n",
      "\n",
      " Aggregating models\n",
      "Cosine Similarities:  [tensor(0.4578), tensor(0.3835), tensor(0.3851), tensor(0.4727), tensor(0.9994), tensor(1.0000)]\n",
      "Aggregation Weights:  [0.08559642 0.09664151 0.09562977 0.06663243 0.2712425  0.27038193]\n",
      "\n",
      "\n",
      "\n",
      "Aggregate test at iter  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:02<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0020, Accuracy: 3527/10000 (35%)\n",
      "\n",
      "\n",
      "\n",
      " Entering epoch 0\n",
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 1 1 2 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([3], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[ 0.28854247  0.13562625  0.13524259  0.10077783  0.09799805  0.09522728]\n",
      " [ 0.13562625  0.28854258  0.28841681 -0.00258547  0.00506725  0.00441333]\n",
      " [ 0.1352426   0.28841681  0.28854202 -0.00249502  0.00512249  0.00448062]\n",
      " [ 0.10077783 -0.00258547 -0.00249502  0.28854198  0.09718428  0.09399347]\n",
      " [ 0.09799805  0.00506725  0.00512248  0.09718428  0.28854271  0.28835465]\n",
      " [ 0.09522728  0.00441333  0.00448063  0.09399347  0.28835465  0.2885433 ]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 1 1 2 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([3], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "\n",
      "\n",
      " Entering epoch 1\n",
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 1 1 2 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([3], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[0.26906885 0.15893372 0.15880608 0.11825676 0.12613722 0.12325314]\n",
      " [0.15893372 0.26906867 0.26893629 0.03530331 0.06865631 0.06718898]\n",
      " [0.15880607 0.26893629 0.26906751 0.03511613 0.06908393 0.06764519]\n",
      " [0.11825676 0.0353033  0.03511613 0.26906852 0.10518016 0.10250397]\n",
      " [0.12613722 0.06865631 0.06908393 0.10518015 0.26906833 0.26886419]\n",
      " [0.12325314 0.06718898 0.06764519 0.10250398 0.26886419 0.26906892]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 1 1 2 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([3], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "\n",
      "\n",
      " Entering epoch 2\n",
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 1 1 2 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([3], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[0.26535387 0.1113535  0.11339875 0.13865573 0.12767665 0.12621102]\n",
      " [0.1113535  0.26535358 0.2650674  0.03985277 0.09191151 0.08953869]\n",
      " [0.11339874 0.26506741 0.26535419 0.03980049 0.09149949 0.08917713]\n",
      " [0.13865574 0.03985277 0.03980049 0.26535392 0.13440878 0.1323396 ]\n",
      " [0.12767665 0.09191151 0.09149949 0.13440879 0.26535376 0.26503796]\n",
      " [0.12621102 0.08953868 0.08917713 0.1323396  0.26503798 0.26535333]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 1 1 2 0 0]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([4, 5], dtype=int64), array([1, 2], dtype=int64), array([3], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "\n",
      "\n",
      " Entering epoch 3\n",
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 0 0 1 2 2]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([3], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[0.26186762 0.08537292 0.09054768 0.14441021 0.12690057 0.12590025]\n",
      " [0.08537291 0.26186813 0.25801907 0.05373258 0.10455481 0.10391999]\n",
      " [0.09054767 0.25801907 0.26186702 0.0592618  0.11499047 0.11423411]\n",
      " [0.14441022 0.05373258 0.0592618  0.26186778 0.1360923  0.13446327]\n",
      " [0.12690056 0.10455481 0.11499047 0.1360923  0.26186758 0.26153796]\n",
      " [0.12590025 0.10391999 0.11423411 0.13446327 0.26153798 0.2618668 ]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 0 0 1 2 2]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([3], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "\n",
      "\n",
      " Entering epoch 4\n",
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 0 0 3 1 1]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[0.2545267  0.09317384 0.10744654 0.1485406  0.14469739 0.14375348]\n",
      " [0.09317384 0.25452581 0.24499    0.06645229 0.0905715  0.09099869]\n",
      " [0.10744655 0.24499    0.25452574 0.07467758 0.10501373 0.10513457]\n",
      " [0.14854061 0.06645229 0.07467758 0.2545262  0.15660818 0.15508553]\n",
      " [0.14469739 0.09057151 0.10501374 0.15660819 0.25452585 0.25416792]\n",
      " [0.14375348 0.09099869 0.10513457 0.15508553 0.25416791 0.25452519]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 0 0 3 1 1]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "Benign net distance [tensor(96.0340), tensor(88.5780), tensor(80.4835), tensor(82.8400), tensor(3.6424), tensor(0.)]\n",
      "Malicious net distance []\n",
      "cos_sim list  [tensor(0.5868), tensor(0.3472), tensor(0.4070), tensor(0.6344), tensor(0.9986), tensor(1.0000)]\n",
      "Norms of local gradients  [tensor(1.2206), tensor(0.9083), tensor(0.8331), tensor(1.0614), tensor(0.6829), tensor(0.6784)]\n",
      "\n",
      " Aggregating models\n",
      "Cosine Similarities:  [tensor(0.5868), tensor(0.3472), tensor(0.4070), tensor(0.6344), tensor(0.9986), tensor(1.0000)]\n",
      "Aggregation Weights:  [0.08206422 0.06525999 0.08339978 0.10203329 0.24960363 0.251637  ]\n",
      "\n",
      "\n",
      "\n",
      "Aggregate test at iter  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:02<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 5451/10000 (55%)\n",
      "\n",
      "\n",
      "\n",
      " Entering epoch 0\n",
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 0 0 1 2 2]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([3], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[ 0.29063625  0.12045495  0.12632758  0.09305164  0.11648102  0.11479987]\n",
      " [ 0.12045496  0.29063596  0.28778679 -0.01251209  0.04426943  0.0440272 ]\n",
      " [ 0.12632757  0.28778677  0.29063592 -0.01196101  0.04924262  0.04877707]\n",
      " [ 0.09305164 -0.01251209 -0.01196101  0.29063647  0.04882095  0.04729281]\n",
      " [ 0.11648103  0.04426943  0.04924262  0.04882095  0.29063509  0.2903026 ]\n",
      " [ 0.11479987  0.0440272   0.04877706  0.04729281  0.2903026   0.29063593]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [3 0 0 2 1 1]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64), array([0], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "\n",
      "\n",
      " Entering epoch 1\n",
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 4 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 5 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 0 0 3 1 1]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "[[ 0.27204508  0.09521172  0.09031917  0.11310009  0.12392015  0.12358303]\n",
      " [ 0.09521172  0.27204547  0.26956033 -0.01359709  0.09496026  0.094972  ]\n",
      " [ 0.09031917  0.26956033  0.27204639 -0.01120258  0.09520769  0.09535226]\n",
      " [ 0.11310009 -0.01359709 -0.01120258  0.2720447   0.13166729  0.13083476]\n",
      " [ 0.12392015  0.09496026  0.0952077   0.1316673   0.27204572  0.27149515]\n",
      " [ 0.12358303  0.094972    0.09535226  0.13083476  0.27149516  0.27204504]]\n",
      "Original Copylist [1, 0, 0, 3, 2, 2]\n",
      "Found clusters [2 0 0 3 1 1]\n",
      "Original groups [array([1, 2], dtype=int64), array([0], dtype=int64), array([4, 5], dtype=int64), array([3], dtype=int64)]\n",
      "Clustered groups [array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([0], dtype=int64), array([3], dtype=int64)]\n",
      "Clustering score 1.0\n",
      "\n",
      "\n",
      " Entering epoch 2\n",
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 0 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 1 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 2 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:03<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 - Epoch 0 - Client Type: Benign - Client Number 3 - Poison Training False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.23s/it]"
     ]
    }
   ],
   "source": [
    "fl = CustomFL(n_iter=n_iter, n_epochs=n_epochs, poison_starts_at_iter=poison_starts_at_iter, num_of_benign_nets=num_of_workers-num_of_mal_workers, num_of_mal_nets=num_of_mal_workers, \n",
    "              inertia=inertia, momentum=momentum,\n",
    "              attack_type=attack_type, scale_up=scale_up, minimizeDist=minimizeDist\n",
    ")\n",
    "fl.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsUXB_dWgKbg"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'file_1_dec27_all_benign_iters_50_epoch_{n_epochs}.txt', 'wb') as f:\n",
    "    pickle.dump(fl.debug_log, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath=os.getcwd()\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi3Zkh_HuFEI"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('file_1_dec23_label_flip_minDist_off.txt', 'rb') as f:\n",
    "    log_file=pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "with open('file_1_dec23_label_flip_minDist_on.txt', 'rb') as f:\n",
    "    log_file_2=pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "log_by_iter=[]\n",
    "for iter in range(n_iter):\n",
    "    log_by_iter.append([])\n",
    "idx=0\n",
    "cluster_logs=[]\n",
    "for l in log_file:\n",
    "    (i, _, _, _)=l\n",
    "    log_by_iter[i].append(l)\n",
    "    if i==-1:\n",
    "        cluster_logs.append(l)\n",
    "\n",
    "\n",
    "log_by_iter_2=[]\n",
    "for iter in range(n_iter):\n",
    "    log_by_iter_2.append([])\n",
    "idx=0\n",
    "cluster_logs_2=[]\n",
    "for l in log_file_2:\n",
    "    (i, _, _, _)=l\n",
    "    log_by_iter_2[i].append(l)\n",
    "    if i==-1:\n",
    "        cluster_logs_2.append(l)\n",
    "\n",
    "\n",
    "cluster_scores=[]\n",
    "for i, log in enumerate(cluster_logs):\n",
    "    if (i+1)%3==0:\n",
    "        (_, _, _, sc) = log\n",
    "        cluster_scores.append(sc)\n",
    "        \n",
    "cluster_scores_2=[]\n",
    "for i, log in enumerate(cluster_logs_2):\n",
    "    if (i+1)%3==0:\n",
    "        (_, _, _, sc) = log\n",
    "        cluster_scores_2.append(sc)\n",
    "        \n",
    "print(cluster_scores, cluster_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6ii0njBC6JT"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [4, 2], [4, 4], [4, 0]])\n",
    "clustering = AgglomerativeClustering(n_clusters=3).fit(X)\n",
    "labels=clustering.labels_\n",
    "print(labels, labels[labels==0], np.argwhere(labels==0))\n",
    "print([np.argwhere(labels==i).flatten() for i in range(3)])\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnSDjbvSjoiL"
   },
   "outputs": [],
   "source": [
    "t_arr=np.array([100, 10, 1])\n",
    "t_arr_sig = 0.1 * t_arr\n",
    "print(np.random.normal(t_arr, t_arr_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zOwHqDgoJx0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yenDNiAaqd_F"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "for i in [0, 3, 5]:\n",
    "    (_, _, tl) = train_loaders[0][i]\n",
    "\n",
    "    print(tl)\n",
    "\n",
    "    # get some random training images\n",
    "    dataiter = iter(tl)\n",
    "    images, labels = dataiter.next()\n",
    "    labels = np.array(labels)\n",
    "    imshow(torchvision.utils.make_grid(images[:16]))\n",
    "    for n in range(10):\n",
    "        print(n, np.count_nonzero(labels == n)/len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjNZrWiG-hPG"
   },
   "outputs": [],
   "source": [
    "sampled_probabilities = 1000 * np.random.dirichlet(\n",
    "                np.array(3 * [0.9]))\n",
    "print(np.sum(sampled_probabilities), sampled_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOdKOaAbFIN7"
   },
   "outputs": [],
   "source": [
    "print(3 * [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGkTxR66cWS9"
   },
   "outputs": [],
   "source": [
    "print(fl.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0VZP12IgvhL"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('file_1_dec16.txt', 'rb') as f:\n",
    "    log_file=pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZTPfmkowaO3"
   },
   "outputs": [],
   "source": [
    "log_by_iter=[]\n",
    "for iter in range(n_iter):\n",
    "    log_by_iter.append([])\n",
    "idx=0\n",
    "for l in log_file:\n",
    "    (i, _, _, _)=l\n",
    "    log_by_iter[i].append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gt2Qp862dFj"
   },
   "outputs": [],
   "source": [
    "for i in range(5,10):\n",
    "    log_by_iter[i]=log_by_iter[i][2:]\n",
    "\n",
    "for l in log_by_iter[0]:\n",
    "    print(l)\n",
    "for l in log_by_iter[5]:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXBqfzL33UKh"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cos_sim_benign=[]\n",
    "cos_sim_mal=[]\n",
    "for i in range(10):\n",
    "    (_, _, _, sim) = log_by_iter[i][2]\n",
    "    cos_sim_benign.append(sim[0])\n",
    "    cos_sim_mal.append(sim[2])\n",
    "\n",
    "plt.plot(cos_sim_benign, c='blue')\n",
    "plt.plot(cos_sim_mal, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Er4MnusO4lkV"
   },
   "outputs": [],
   "source": [
    "aggr_weight_benign=[]\n",
    "aggr_weight_mal=[]\n",
    "for i in range(10):\n",
    "    (_, _, _, sim) = log_by_iter[i][4]\n",
    "    aggr_weight_benign.append(sim[0])\n",
    "    aggr_weight_mal.append(sim[2])\n",
    "\n",
    "plt.plot(aggr_weight_benign, c='blue')\n",
    "plt.plot(aggr_weight_mal, c='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcWWK14Z49C7"
   },
   "outputs": [],
   "source": [
    "distance_benign=[]\n",
    "distance_mal=[]\n",
    "for i in range(10):\n",
    "    (_, _, _, sim) = log_by_iter[i][0]\n",
    "    distance_benign.append(sim[0])\n",
    "    (_, _, _, sim) = log_by_iter[i][1]\n",
    "    distance_mal.append(sim)\n",
    "\n",
    "plt.plot(distance_benign, c='blue')\n",
    "plt.plot(distance_mal, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CH-InCz25jkj"
   },
   "outputs": [],
   "source": [
    "accuracy=[]\n",
    "for i in range(10):\n",
    "    (_, _, _, sim) = log_by_iter[i][5]\n",
    "    accuracy.append(sim)\n",
    "\n",
    "plt.plot(accuracy, c='blue')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "custom optimizer 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
